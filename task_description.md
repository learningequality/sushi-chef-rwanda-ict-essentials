Scraping take home project
==========================

Hello!

The purpose of this project is to get you familiar with the tools and procedures
for downloading educational web resources from the web. We would like you to
download all the educational resources (`mp3`, `mp4`, `pdf` files) and the associated
metadata from a the [sample website](http://chef-take-home-test.learningequality.org/).

**Note:** Your scraper code doesn't need to be general purpose or reusable for
other sites: it is a single-purpose scraper specific to the sample website.

The steps to follow are:
  - Download the sample code and instructions from this link:
    https://github.com/fle-internal/chef-take-home-test/archive/0.3.zip
    - Follow the installation instructions provided in [README.md](./README.md).
  - Explore the [sample website]((http://chef-take-home-test.learningequality.org/))
    using your browser:
    - Use the "view source" and "inspect element" tools in your browser to become
      familiar with the `class` and `id` attributes
  - Complete the `scrape_source` function in the script `souschef.py`.
    - If you get stuck, you can check out the code for the sample scrapers in the `examples/` folder
  - Submit your solution by email
    - Send us the updated `souschef.py` script and a link to the output zip file
      generated by your scraper.




Task description
----------------
A sous chef script is responsible for scraping content from a source and putting
it into a folder and csv structure (see example `examples/Sample Channel.zip`).
A sous chef skeleton script has been started for you in [`souschef.py`](./souschef.py).

You are required to submit two deliverables for this project:
  1. The updated python script `souschef.py` that performs the scraping.
  2. A `zip` file of the scraping results that consists of three parts:
     - The nested tree of folders with media files (`pdf`, `mp4`, `mp3`)
     - The channel metadata file `Channel.csv`
     - The content metadata file `Content.csv`

The contents of the zip file look like this:

    containing_folder/
        Content.csv
        Channel.csv
        sample_channel/
            topic1/
                content1.mp3
                content2.mp4
            topic2/
                content3.pdf

Each line in the file `Content.csv` corresponds to one content node in the folder
structure and it's columns contain the attributes for that node.

Don't worry, **you won't have to generate the CSVs and directories by hand**:
we've prepared some helper methods (in the `utils` folder) to help you produce
an archive in the right format.

The [README file](./README.md) provides additional info for the Python tools you
have at your disposal to help with your work. Our estimate is that it shouldn't
take you more than four hours to complete the task.

**TODO** You can check if the metadata files `Channel.csv` and `Content.csv` are
by running the script `python chef.py --check` **TODO Oct 12**



Context
-------
The following diagram will help you better understand how the Sous Chef scraping
scripts fit in the context of the Kolibri content pipeline:

     scraping      →  package content    →   store/edit content   →   import content
     souschef.py      chef.py                Kolibri Studio           Kolibri

1. Everything starts with a "Sous Chef" scripts (what you'll be working on) that
   download content and metadata from various educational websites, stores all
   content items in a folder hierarchy and the associated metadata.
2. In the next step a "Chef script" takes over to upload the content and create
   the channel to Kolibri Studio. Chef scripts use the python package called [ricecooker](https://github.com/learningequality/ricecooker/).
3. The [Kolibri Studio](https://studio.learningequality.org) is a server operated
   by Learning Equality that contains all the channels. A channel is a set of
   content from a content provider, e.g. Khan Academy, that is structured through
   a tree of topics. Any user can log in and manually create and curate their own channel.
4. All this work is so that teachers and administrators can easily import educational
   content from the Kolibri applications.
